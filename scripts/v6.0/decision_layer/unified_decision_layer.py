#!/usr/bin/env python3
"""
Quant-Investor V6.0 - ç»Ÿä¸€å†³ç­–å±‚ (Unified Decision Layer)

æ•´åˆæ‰€æœ‰å†å²ç‰ˆæœ¬çš„å®šæ€§åˆ†æèƒ½åŠ›ï¼š
- V2.9: å¤šAgentè¾©è®ºç³»ç»Ÿ (è´¢åŠ¡/è¡Œä¸š/æŠ¤åŸæ²³/ä¼°å€¼/é£é™© 5å¤§ä¸“å®¶)
- V3.6: å¤šLLMé€‚é…å™¨ (OpenAI/Gemini/DeepSeek/Qwen/Kimi)
- V4.0: å®šæ€§åˆ†æä¸ä¼°å€¼ (DCF/åå‘DCF/å¯æ¯”å…¬å¸)

è®¾è®¡åŸåˆ™ï¼š
1. è‡ªåŠ¨æ£€æµ‹å¯ç”¨LLMå¹¶é€‰æ‹©æœ€ä¼˜
2. å¤šAgentç‹¬ç«‹åˆ†æ â†’ äº¤å‰è´¨è¯¢ â†’ ç»¼åˆç»“è®º
3. ç»“æ„åŒ–è¾“å‡ºï¼Œä¾¿äºä¸é‡åŒ–ä¿¡å·èåˆ
4. å®‰å…¨çš„APIå¯†é’¥ç®¡ç†
"""

import os
import sys
import json
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime
from abc import ABC, abstractmethod
from enum import Enum


# ==================== LLMé€‚é…å™¨ (æºè‡ªV3.6) ====================

class LLMProvider(str, Enum):
    OPENAI = "openai"
    GEMINI = "gemini"
    DEEPSEEK = "deepseek"
    QWEN = "qwen"
    KIMI = "kimi"


PROVIDER_CONFIGS = {
    LLMProvider.OPENAI: {
        "base_url": None,
        "env_key": "OPENAI_API_KEY",
        "default_model": "gpt-4o",
        "models": ["gpt-5", "gpt-4o", "gpt-4o-mini", "gpt-4-turbo", "o1", "o1-mini"]
    },
    LLMProvider.GEMINI: {
        "base_url": None,
        "env_key": "GEMINI_API_KEY",
        "default_model": "gemini-2.5-flash",
        "models": ["gemini-2.5-flash", "gemini-2.0-flash", "gemini-1.5-pro"]
    },
    LLMProvider.DEEPSEEK: {
        "base_url": "https://api.deepseek.com",
        "env_key": "DEEPSEEK_API_KEY",
        "default_model": "deepseek-chat",
        "models": ["deepseek-chat", "deepseek-reasoner"]
    },
    LLMProvider.QWEN: {
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "env_key": "DASHSCOPE_API_KEY",
        "default_model": "qwen-plus",
        "models": ["qwen-turbo", "qwen-plus", "qwen-max", "qwen3-max"]
    },
    LLMProvider.KIMI: {
        "base_url": "https://api.moonshot.cn/v1",
        "env_key": "MOONSHOT_API_KEY",
        "default_model": "moonshot-v1-8k",
        "models": ["moonshot-v1-8k", "moonshot-v1-32k", "moonshot-v1-128k"]
    }
}


@dataclass
class LLMResponse:
    content: str
    model: str
    provider: str
    latency_ms: float = 0.0
    tokens_used: int = 0


class LLMAdapter:
    """
    ç»Ÿä¸€LLMé€‚é…å™¨ (æºè‡ªV3.6ï¼Œå¢å¼ºç‰ˆ)
    
    è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„LLMæä¾›å•†ï¼Œæä¾›ç»Ÿä¸€çš„å¯¹è¯æ¥å£ã€‚
    """
    
    def __init__(self, preferred_providers: List[str] = None, verbose: bool = True):
        self.verbose = verbose
        self.preferred_providers = preferred_providers or ["gemini", "openai", "deepseek", "qwen"]
        self._load_credentials()
        self._available_adapters = {}
        self._init_adapters()
    
    def _load_credentials(self):
        """ä»å®‰å…¨å­˜å‚¨åŠ è½½APIå¯†é’¥"""
        cred_path = os.path.expanduser("~/.quant_investor/credentials.env")
        if os.path.exists(cred_path):
            with open(cred_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#') and '=' in line:
                        key, value = line.split('=', 1)
                        key, value = key.strip(), value.strip()
                        if value and not value.startswith('your_') and '...' not in value:
                            os.environ[key] = value
    
    def _init_adapters(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„LLMé€‚é…å™¨"""
        for provider_name in self.preferred_providers:
            try:
                provider = LLMProvider(provider_name.lower())
                config = PROVIDER_CONFIGS[provider]
                api_key = os.getenv(config["env_key"], "")
                
                if not api_key or len(api_key) < 10:
                    continue
                
                if provider == LLMProvider.GEMINI:
                    self._init_gemini(provider, config, api_key)
                else:
                    self._init_openai_compatible(provider, config, api_key)
                    
            except Exception as e:
                if self.verbose:
                    print(f"  âš ï¸ {provider_name} åˆå§‹åŒ–å¤±è´¥: {e}")
        
        if self.verbose:
            print(f"  âœ… å¯ç”¨LLM: {list(self._available_adapters.keys())}")
    
    def _init_gemini(self, provider, config, api_key):
        """åˆå§‹åŒ–Geminié€‚é…å™¨"""
        try:
            from google import genai
            client = genai.Client(api_key=api_key)
            self._available_adapters[provider.value] = {
                'type': 'gemini', 'client': client,
                'model': config['default_model']
            }
            if self.verbose:
                print(f"  âœ… Gemini åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            pass
    
    def _init_openai_compatible(self, provider, config, api_key):
        """åˆå§‹åŒ–OpenAIå…¼å®¹é€‚é…å™¨"""
        try:
            from openai import OpenAI
            
            base_url = config.get("base_url")
            if provider == LLMProvider.OPENAI:
                base_url = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
            
            client = OpenAI(api_key=api_key, base_url=base_url)
            self._available_adapters[provider.value] = {
                'type': 'openai', 'client': client,
                'model': config['default_model']
            }
            if self.verbose:
                print(f"  âœ… {provider.value} åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            pass
    
    def chat(self, messages: List[Dict[str, str]], provider: str = None,
             temperature: float = 0.3, max_tokens: int = 4000) -> LLMResponse:
        """
        å‘é€èŠå¤©è¯·æ±‚
        
        è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„LLMï¼Œæ”¯æŒæ•…éšœè½¬ç§»ã€‚
        """
        providers_to_try = []
        if provider and provider in self._available_adapters:
            providers_to_try.append(provider)
        providers_to_try.extend([p for p in self._available_adapters if p not in providers_to_try])
        
        for prov in providers_to_try:
            try:
                adapter = self._available_adapters[prov]
                start = time.time()
                
                if adapter['type'] == 'gemini':
                    response = self._call_gemini(adapter, messages, temperature, max_tokens)
                else:
                    response = self._call_openai(adapter, messages, temperature, max_tokens)
                
                latency = (time.time() - start) * 1000
                return LLMResponse(
                    content=response, model=adapter['model'],
                    provider=prov, latency_ms=latency
                )
            except Exception as e:
                if self.verbose:
                    print(f"    âš ï¸ {prov} è°ƒç”¨å¤±è´¥: {e}")
                continue
        
        # æ‰€æœ‰LLMéƒ½å¤±è´¥ï¼Œè¿”å›æ¨¡æ‹Ÿå“åº”
        return LLMResponse(
            content=self._get_mock_response(messages),
            model="mock", provider="mock", latency_ms=0
        )
    
    def _call_gemini(self, adapter, messages, temperature, max_tokens):
        """è°ƒç”¨Gemini"""
        from google.genai import types
        
        contents = []
        system_instruction = None
        
        for msg in messages:
            if msg["role"] == "system":
                system_instruction = msg["content"]
            elif msg["role"] == "user":
                contents.append(types.Content(
                    role="user", parts=[types.Part(text=msg["content"])]
                ))
            elif msg["role"] == "assistant":
                contents.append(types.Content(
                    role="model", parts=[types.Part(text=msg["content"])]
                ))
        
        config = types.GenerateContentConfig(
            temperature=temperature,
            max_output_tokens=max_tokens,
            system_instruction=system_instruction
        )
        
        response = adapter['client'].models.generate_content(
            model=adapter['model'], contents=contents, config=config
        )
        return response.text
    
    def _call_openai(self, adapter, messages, temperature, max_tokens):
        """è°ƒç”¨OpenAIå…¼å®¹API"""
        response = adapter['client'].chat.completions.create(
            model=adapter['model'],
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content
    
    def _get_mock_response(self, messages):
        """æ¨¡æ‹Ÿå“åº”ï¼ˆæ‰€æœ‰LLMä¸å¯ç”¨æ—¶ï¼‰"""
        return json.dumps({
            "summary": "LLMæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¿”å›æ¨¡æ‹Ÿåˆ†æç»“æœ",
            "score": 6.0, "confidence": 0.5,
            "conclusion": "å»ºè®®åœ¨LLMæœåŠ¡æ¢å¤åé‡æ–°è¿è¡Œåˆ†æ",
            "key_findings": [{"finding": "éœ€è¦LLMæœåŠ¡æ”¯æŒæ·±åº¦åˆ†æ"}],
            "risks": ["LLMæœåŠ¡ä¸å¯ç”¨"], "opportunities": ["å¾…åˆ†æ"]
        }, ensure_ascii=False)
    
    def get_available_providers(self) -> List[str]:
        return list(self._available_adapters.keys())
    
    def is_available(self) -> bool:
        return len(self._available_adapters) > 0


# ==================== åˆ†æAgent (æºè‡ªV2.9) ====================

@dataclass
class AgentAnalysis:
    """Agentåˆ†æç»“æœ"""
    agent_name: str
    agent_role: str
    score: float = 5.0
    confidence: float = 0.5
    summary: str = ""
    conclusion: str = ""
    key_findings: List[Dict] = field(default_factory=list)
    risks: List[str] = field(default_factory=list)
    opportunities: List[str] = field(default_factory=list)
    detailed_analysis: Dict = field(default_factory=dict)


class AnalysisAgent:
    """é€šç”¨åˆ†æAgent"""
    
    def __init__(self, name: str, role: str, system_prompt: str, llm: LLMAdapter):
        self.name = name
        self.role = role
        self.system_prompt = system_prompt
        self.llm = llm
    
    def analyze(self, stock_code: str, company_name: str, 
                data_summary: str, quant_summary: str = "") -> AgentAnalysis:
        """æ‰§è¡Œåˆ†æ"""
        user_prompt = f"""è¯·å¯¹ä»¥ä¸‹è‚¡ç¥¨è¿›è¡Œ{self.role}åˆ†æï¼š

## è‚¡ç¥¨ä¿¡æ¯
- ä»£ç : {stock_code}
- åç§°: {company_name}

## æ•°æ®æ‘˜è¦
{data_summary}

## é‡åŒ–åˆ†æç»“æœ
{quant_summary}

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- summary: ä¸€å¥è¯æ€»ç»“
- score: è¯„åˆ† (1-10, 10ä¸ºæœ€ä½³)
- confidence: ç½®ä¿¡åº¦ (0-1)
- conclusion: è¯¦ç»†ç»“è®º (2-3æ®µ)
- key_findings: å…³é”®å‘ç°åˆ—è¡¨ [{{"finding": "...", "evidence": "...", "impact": "é«˜/ä¸­/ä½"}}]
- risks: é£é™©åˆ—è¡¨
- opportunities: æœºä¼šåˆ—è¡¨
"""
        
        response = self.llm.chat([
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": user_prompt}
        ])
        
        return self._parse_response(response.content)
    
    def rebut(self, questions: List[str], original_analysis: AgentAnalysis) -> Dict:
        """å›åº”è´¨è¯¢"""
        prompt = f"""ä½ ä¹‹å‰å¯¹è¯¥è‚¡ç¥¨çš„åˆ†æç»“è®ºæ˜¯ï¼š
{original_analysis.conclusion}
è¯„åˆ†: {original_analysis.score}/10

ç°åœ¨ä¸»æŒäººæå‡ºäº†ä»¥ä¸‹è´¨ç–‘ï¼š
{chr(10).join(f'{i+1}. {q}' for i, q in enumerate(questions))}

è¯·è®¤çœŸå›åº”æ¯ä¸ªé—®é¢˜ï¼Œå¦‚æœ‰å¿…è¦å¯ä»¥è°ƒæ•´ä½ çš„è§‚ç‚¹ã€‚
ä»¥JSONæ ¼å¼è¿”å›ï¼š
- responses: å¯¹æ¯ä¸ªé—®é¢˜çš„å›åº”åˆ—è¡¨
- stance_changed: æ˜¯å¦è°ƒæ•´è§‚ç‚¹ (true/false)
- new_score: è°ƒæ•´åçš„è¯„åˆ† (å¦‚æœè°ƒæ•´äº†)
"""
        
        response = self.llm.chat([
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": prompt}
        ])
        
        try:
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', response.content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
            return json.loads(response.content)
        except:
            return {"responses": [response.content], "stance_changed": False}
    
    def _parse_response(self, content: str) -> AgentAnalysis:
        """è§£æLLMå“åº”"""
        import re
        
        try:
            json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group(1))
            else:
                parsed = json.loads(content)
        except:
            parsed = {
                "summary": content[:200],
                "score": 6.0,
                "confidence": 0.5,
                "conclusion": content[:1000],
                "key_findings": [],
                "risks": [],
                "opportunities": []
            }
        
        return AgentAnalysis(
            agent_name=self.name,
            agent_role=self.role,
            score=float(parsed.get("score", 6.0)),
            confidence=float(parsed.get("confidence", 0.5)),
            summary=parsed.get("summary", ""),
            conclusion=parsed.get("conclusion", ""),
            key_findings=parsed.get("key_findings", []),
            risks=parsed.get("risks", []),
            opportunities=parsed.get("opportunities", []),
            detailed_analysis=parsed
        )


# ==================== è¾©è®ºå¼•æ“ (æºè‡ªV2.9) ====================

@dataclass
class DebateResult:
    """è¾©è®ºç»“æœ"""
    stock_code: str
    company_name: str
    
    # å„Agentåˆ†æ
    agent_analyses: Dict[str, AgentAnalysis] = field(default_factory=dict)
    
    # è¾©è®ºè½®æ¬¡è®°å½•
    debate_rounds: List[Dict] = field(default_factory=list)
    
    # æœ€ç»ˆç»“è®º
    final_score: float = 5.0
    final_confidence: float = 0.5
    investment_rating: str = "æŒæœ‰"
    consensus: str = ""
    bull_case: str = ""
    bear_case: str = ""
    
    # ä¼°å€¼
    valuation_summary: str = ""
    
    # å…ƒä¿¡æ¯
    duration_seconds: float = 0.0
    llm_providers_used: List[str] = field(default_factory=list)


class DebateEngine:
    """
    å¤šAgentè¾©è®ºå¼•æ“ (æºè‡ªV2.9ï¼Œå¢å¼ºç‰ˆ)
    
    æµç¨‹ï¼š
    1. å„ä¸“å®¶Agentç‹¬ç«‹åˆ†æ
    2. ä¸»æŒäººç»„ç»‡äº¤å‰è´¨è¯¢
    3. ç»¼åˆå½¢æˆæœ€ç»ˆç»“è®º
    """
    
    def __init__(self, llm: LLMAdapter, max_rounds: int = 1, verbose: bool = True):
        self.llm = llm
        self.max_rounds = max_rounds
        self.verbose = verbose
        
        # åˆ›å»º5å¤§ä¸“å®¶Agent
        self.agents = self._create_agents()
    
    def _create_agents(self) -> List[AnalysisAgent]:
        """åˆ›å»ºä¸“å®¶Agentå›¢é˜Ÿ"""
        agents = [
            AnalysisAgent("è´¢åŠ¡åˆ†æå¸ˆ", "è´¢åŠ¡åˆ†æ", 
                """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è´¢åŠ¡åˆ†æå¸ˆï¼Œä¸“æ³¨äºï¼š
                - è´¢åŠ¡æŠ¥è¡¨åˆ†æï¼ˆä¸‰è¡¨è”åŠ¨ï¼‰
                - ç›ˆåˆ©èƒ½åŠ›ï¼ˆROEã€æ¯›åˆ©ç‡ã€å‡€åˆ©ç‡ï¼‰
                - æˆé•¿æ€§ï¼ˆè¥æ”¶å¢é€Ÿã€åˆ©æ¶¦å¢é€Ÿï¼‰
                - ç°é‡‘æµè´¨é‡
                - èµ„äº§è´Ÿå€ºç»“æ„
                è¯·åŸºäºæ•°æ®ç»™å‡ºå®¢è§‚ã€é‡åŒ–çš„åˆ†æã€‚""", self.llm),
            
            AnalysisAgent("è¡Œä¸šåˆ†æå¸ˆ", "è¡Œä¸šåˆ†æ",
                """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è¡Œä¸šåˆ†æå¸ˆï¼Œä¸“æ³¨äºï¼š
                - è¡Œä¸šå‘å±•è¶‹åŠ¿å’Œå¸‚åœºç©ºé—´
                - ç«äº‰æ ¼å±€å’Œå¸‚åœºä»½é¢
                - è¡Œä¸šæ”¿ç­–å’Œç›‘ç®¡ç¯å¢ƒ
                - æŠ€æœ¯å˜é©å’Œåˆ›æ–°é©±åŠ¨
                - ä¸Šä¸‹æ¸¸äº§ä¸šé“¾åˆ†æ
                è¯·ç»“åˆè¡Œä¸šæ•°æ®ç»™å‡ºæ·±å…¥åˆ†æã€‚""", self.llm),
            
            AnalysisAgent("æŠ¤åŸæ²³åˆ†æå¸ˆ", "æŠ¤åŸæ²³åˆ†æ",
                """ä½ æ˜¯ä¸€ä½ä¸“æ³¨äºä¼ä¸šç«äº‰ä¼˜åŠ¿çš„åˆ†æå¸ˆï¼Œä¸“æ³¨äºï¼š
                - å“ç‰Œä»·å€¼å’Œå®¢æˆ·ç²˜æ€§
                - ç½‘ç»œæ•ˆåº”å’Œè§„æ¨¡ç»æµ
                - æŠ€æœ¯å£å’å’Œä¸“åˆ©ä¿æŠ¤
                - è½¬æ¢æˆæœ¬å’Œé”å®šæ•ˆåº”
                - æˆæœ¬ä¼˜åŠ¿å’Œèµ„æºå„æ–­
                è¯·è¯„ä¼°ä¼ä¸šæŠ¤åŸæ²³çš„å®½åº¦å’ŒæŒä¹…æ€§ã€‚""", self.llm),
            
            AnalysisAgent("ä¼°å€¼åˆ†æå¸ˆ", "ä¼°å€¼åˆ†æ",
                """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¼°å€¼åˆ†æå¸ˆï¼Œä¸“æ³¨äºï¼š
                - DCFä¼°å€¼ï¼ˆè‡ªç”±ç°é‡‘æµæŠ˜ç°ï¼‰
                - åå‘DCFï¼ˆå¸‚åœºéšå«çš„å¢é•¿é¢„æœŸï¼‰
                - å¯æ¯”å…¬å¸ä¼°å€¼ï¼ˆPE/PB/PS/EV-EBITDAï¼‰
                - å†å²ä¼°å€¼åŒºé—´åˆ†æ
                - å®‰å…¨è¾¹é™…è¯„ä¼°
                è¯·ç»™å‡ºå…·ä½“çš„ä¼°å€¼åŒºé—´å’Œå®‰å…¨è¾¹é™…ã€‚""", self.llm),
            
            AnalysisAgent("é£é™©åˆ†æå¸ˆ", "é£é™©åˆ†æ",
                """ä½ æ˜¯ä¸€ä½ä¸“æ³¨äºé£é™©ç®¡ç†çš„åˆ†æå¸ˆï¼Œä¸“æ³¨äºï¼š
                - ç³»ç»Ÿæ€§é£é™©ï¼ˆå®è§‚ç»æµã€æ”¿ç­–ï¼‰
                - ç»è¥é£é™©ï¼ˆç®¡ç†å±‚ã€æˆ˜ç•¥ï¼‰
                - è´¢åŠ¡é£é™©ï¼ˆæ æ†ã€æµåŠ¨æ€§ï¼‰
                - å¸‚åœºé£é™©ï¼ˆä¼°å€¼æ³¡æ²«ã€æµåŠ¨æ€§ï¼‰
                - é»‘å¤©é¹…äº‹ä»¶å’Œå°¾éƒ¨é£é™©
                è¯·å…¨é¢è¯„ä¼°å„ç±»é£é™©åŠå…¶æ¦‚ç‡å’Œå½±å“ã€‚""", self.llm),
        ]
        return agents
    
    def run_debate(self, stock_code: str, company_name: str,
                    data_summary: str, quant_summary: str = "") -> DebateResult:
        """
        è¿è¡Œå®Œæ•´çš„è¾©è®ºæµç¨‹
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            company_name: å…¬å¸åç§°
            data_summary: æ•°æ®æ‘˜è¦
            quant_summary: é‡åŒ–åˆ†ææ‘˜è¦
        
        Returns:
            DebateResult: è¾©è®ºç»“æœ
        """
        start_time = time.time()
        result = DebateResult(stock_code=stock_code, company_name=company_name)
        
        if self.verbose:
            print(f"\n  ğŸ¯ å¼€å§‹è¾©è®º: {company_name} ({stock_code})")
        
        # Round 0: å„Agentç‹¬ç«‹åˆ†æ
        if self.verbose:
            print(f"    Round 0: å„ä¸“å®¶ç‹¬ç«‹åˆ†æ")
        
        for agent in self.agents:
            if self.verbose:
                print(f"      [{agent.name}] åˆ†æä¸­...")
            
            try:
                analysis = agent.analyze(stock_code, company_name, data_summary, quant_summary)
                result.agent_analyses[agent.name] = analysis
                
                if self.verbose:
                    print(f"      [{agent.name}] è¯„åˆ†: {analysis.score}/10, ç½®ä¿¡åº¦: {analysis.confidence:.0%}")
            except Exception as e:
                if self.verbose:
                    print(f"      [{agent.name}] åˆ†æå¤±è´¥: {e}")
                result.agent_analyses[agent.name] = AgentAnalysis(
                    agent_name=agent.name, agent_role=agent.role,
                    summary=f"åˆ†æå¤±è´¥: {e}"
                )
        
        # Round 1-N: äº¤å‰è´¨è¯¢
        for round_num in range(1, self.max_rounds + 1):
            if self.verbose:
                print(f"    Round {round_num}: äº¤å‰è´¨è¯¢")
            
            questions = self._generate_questions(result.agent_analyses)
            rebuttals = {}
            
            for agent in self.agents:
                if agent.name in questions and questions[agent.name]:
                    try:
                        original = result.agent_analyses.get(agent.name)
                        if original:
                            rebuttal = agent.rebut(questions[agent.name], original)
                            rebuttals[agent.name] = rebuttal
                            
                            # æ›´æ–°è¯„åˆ†
                            if rebuttal.get('stance_changed') and rebuttal.get('new_score'):
                                result.agent_analyses[agent.name].score = float(rebuttal['new_score'])
                    except Exception:
                        pass
            
            result.debate_rounds.append({
                'round': round_num,
                'questions': questions,
                'rebuttals': rebuttals
            })
        
        # Final: ç»¼åˆç»“è®º
        if self.verbose:
            print(f"    Final: ç»¼åˆå½¢æˆç»“è®º")
        
        self._synthesize_conclusion(result, data_summary)
        
        result.duration_seconds = time.time() - start_time
        result.llm_providers_used = self.llm.get_available_providers()
        
        if self.verbose:
            print(f"  âœ… è¾©è®ºå®Œæˆ: {result.investment_rating}, "
                  f"è¯„åˆ†: {result.final_score:.1f}/10, "
                  f"è€—æ—¶: {result.duration_seconds:.1f}s")
        
        return result
    
    def _generate_questions(self, analyses: Dict[str, AgentAnalysis]) -> Dict[str, List[str]]:
        """ç”Ÿæˆäº¤å‰è´¨è¯¢é—®é¢˜"""
        questions = {}
        
        # æ‰¾å‡ºåˆ†æ­§æœ€å¤§çš„è§‚ç‚¹
        scores = {name: a.score for name, a in analyses.items()}
        if not scores:
            return questions
        
        max_score = max(scores.values())
        min_score = min(scores.values())
        
        for name, analysis in analyses.items():
            qs = []
            if analysis.score == max_score and max_score - min_score > 2:
                qs.append(f"ä½ çš„è¯„åˆ†({analysis.score})æ˜¯æœ€é«˜çš„ï¼Œæ˜¯å¦è¿‡äºä¹è§‚ï¼Ÿè¯·æä¾›æ›´å¤šè¯æ®æ”¯æŒã€‚")
            elif analysis.score == min_score and max_score - min_score > 2:
                qs.append(f"ä½ çš„è¯„åˆ†({analysis.score})æ˜¯æœ€ä½çš„ï¼Œæ˜¯å¦è¿‡äºæ‚²è§‚ï¼Ÿå…¶ä»–åˆ†æå¸ˆçœ‹åˆ°äº†å“ªäº›ä½ å¿½ç•¥çš„ç§¯æå› ç´ ï¼Ÿ")
            
            if analysis.risks:
                qs.append(f"ä½ æåˆ°çš„é£é™©'{analysis.risks[0]}'ï¼Œæœ‰å¤šå¤§æ¦‚ç‡å‘ç”Ÿï¼Ÿå½±å“ç¨‹åº¦å¦‚ä½•ï¼Ÿ")
            
            if qs:
                questions[name] = qs
        
        return questions
    
    def _synthesize_conclusion(self, result: DebateResult, data_summary: str):
        """ç»¼åˆå½¢æˆæœ€ç»ˆç»“è®º"""
        # è®¡ç®—åŠ æƒå¹³å‡åˆ†
        scores = []
        for analysis in result.agent_analyses.values():
            scores.append(analysis.score * analysis.confidence)
        
        if scores:
            result.final_score = sum(scores) / sum(a.confidence for a in result.agent_analyses.values())
            result.final_confidence = sum(a.confidence for a in result.agent_analyses.values()) / len(result.agent_analyses)
        
        # ç¡®å®šæŠ•èµ„è¯„çº§
        if result.final_score >= 8:
            result.investment_rating = "å¼ºçƒˆä¹°å…¥"
        elif result.final_score >= 7:
            result.investment_rating = "ä¹°å…¥"
        elif result.final_score >= 5:
            result.investment_rating = "æŒæœ‰"
        elif result.final_score >= 3:
            result.investment_rating = "å‡ä»“"
        else:
            result.investment_rating = "å–å‡º"
        
        # ä½¿ç”¨LLMç”Ÿæˆç»¼åˆç»“è®º
        analyses_text = ""
        for name, analysis in result.agent_analyses.items():
            analyses_text += f"\n### {name} (è¯„åˆ†: {analysis.score}/10)\n{analysis.summary}\n"
            if analysis.risks:
                analyses_text += f"é£é™©: {', '.join(analysis.risks[:3])}\n"
            if analysis.opportunities:
                analyses_text += f"æœºä¼š: {', '.join(analysis.opportunities[:3])}\n"
        
        synthesis_prompt = f"""ä½œä¸ºæŠ•èµ„å§”å‘˜ä¼šä¸»æŒäººï¼Œè¯·ç»¼åˆä»¥ä¸‹å„ä¸“å®¶çš„åˆ†æï¼Œå½¢æˆæœ€ç»ˆç»“è®ºï¼š

{analyses_text}

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
- consensus: ç»¼åˆç»“è®º (3-5å¥è¯)
- bull_case: å¤šæ–¹æ ¸å¿ƒè§‚ç‚¹ (2-3å¥è¯)
- bear_case: ç©ºæ–¹æ ¸å¿ƒè§‚ç‚¹ (2-3å¥è¯)
- valuation_summary: ä¼°å€¼æ€»ç»“ (2-3å¥è¯)
"""
        
        try:
            response = self.llm.chat([
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„æŠ•èµ„å§”å‘˜ä¼šä¸»æŒäººï¼Œæ“…é•¿ç»¼åˆå¤šæ–¹è§‚ç‚¹å½¢æˆå®¢è§‚ç»“è®ºã€‚"},
                {"role": "user", "content": synthesis_prompt}
            ])
            
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', response.content, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group(1))
            else:
                parsed = json.loads(response.content)
            
            result.consensus = parsed.get("consensus", "")
            result.bull_case = parsed.get("bull_case", "")
            result.bear_case = parsed.get("bear_case", "")
            result.valuation_summary = parsed.get("valuation_summary", "")
        except Exception:
            # ç®€å•æ±‡æ€»
            result.consensus = f"ç»¼åˆ{len(result.agent_analyses)}ä½ä¸“å®¶åˆ†æï¼Œç»¼åˆè¯„åˆ†{result.final_score:.1f}/10ï¼Œè¯„çº§ï¼š{result.investment_rating}"
            result.bull_case = "; ".join(
                a.opportunities[0] if a.opportunities else "" 
                for a in result.agent_analyses.values()
            )[:500]
            result.bear_case = "; ".join(
                a.risks[0] if a.risks else "" 
                for a in result.agent_analyses.values()
            )[:500]


# ==================== ç»Ÿä¸€å†³ç­–å±‚ ====================

@dataclass
class DecisionLayerOutput:
    """å†³ç­–å±‚å®Œæ•´è¾“å‡º"""
    # å„è‚¡ç¥¨çš„è¾©è®ºç»“æœ
    debate_results: Dict[str, DebateResult] = field(default_factory=dict)
    
    # æœ€ç»ˆæ¨è
    final_recommendations: List[Dict[str, Any]] = field(default_factory=list)
    
    # ç»Ÿè®¡
    stats: Dict[str, Any] = field(default_factory=dict)


class UnifiedDecisionLayer:
    """
    V6.0 ç»Ÿä¸€å†³ç­–å±‚
    
    å¯¹å€™é€‰è‚¡ç¥¨è¿›è¡Œæ·±å…¥çš„å®šæ€§åˆ†æå’Œå¤šAgentè¾©è®ºã€‚
    """
    
    def __init__(self, llm_preference: List[str] = None, verbose: bool = True,
                  max_debate_rounds: int = 1):
        self.verbose = verbose
        self.max_debate_rounds = max_debate_rounds
        
        if self.verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ§  V6.0 ç»Ÿä¸€å†³ç­–å±‚åˆå§‹åŒ–")
            print(f"{'='*60}")
        
        # åˆå§‹åŒ–LLMé€‚é…å™¨
        self.llm = LLMAdapter(preferred_providers=llm_preference, verbose=verbose)
        
        # åˆå§‹åŒ–è¾©è®ºå¼•æ“
        self.debate_engine = DebateEngine(
            llm=self.llm, max_rounds=max_debate_rounds, verbose=verbose
        )
    
    def analyze(self, ranked_stocks: List[Dict], data_bundle=None,
                quant_summary: str = "", max_stocks: int = 5) -> DecisionLayerOutput:
        """
        å¯¹æ’åé å‰çš„å€™é€‰è‚¡ç¥¨è¿›è¡Œæ·±åº¦å®šæ€§åˆ†æ
        
        Args:
            ranked_stocks: æ’åºåçš„å€™é€‰è‚¡ç¥¨ (æ¥è‡ªæ¨¡å‹å±‚)
            data_bundle: æ•°æ®åŒ… (æ¥è‡ªæ•°æ®å±‚)
            quant_summary: é‡åŒ–åˆ†ææ‘˜è¦
            max_stocks: æœ€å¤šåˆ†æçš„è‚¡ç¥¨æ•°é‡
        
        Returns:
            DecisionLayerOutput: å†³ç­–å±‚å®Œæ•´è¾“å‡º
        """
        output = DecisionLayerOutput()
        
        if self.verbose:
            print(f"\n{'='*60}")
            print(f"ğŸ§  V6.0 ç»Ÿä¸€å†³ç­–å±‚")
            print(f"{'='*60}")
        
        if not self.llm.is_available():
            if self.verbose:
                print(f"  âš ï¸ æ— å¯ç”¨LLMï¼Œè·³è¿‡å®šæ€§åˆ†æ")
            output.final_recommendations = ranked_stocks[:max_stocks]
            return output
        
        # å¯¹Top Nè‚¡ç¥¨è¿›è¡Œè¾©è®º
        stocks_to_analyze = ranked_stocks[:max_stocks]
        
        for i, stock in enumerate(stocks_to_analyze, 1):
            code = stock.get('code', '')
            name = stock.get('name', code)
            
            if self.verbose:
                print(f"\n  {'â”€'*40}")
                print(f"  [{i}/{len(stocks_to_analyze)}] åˆ†æ: {name} ({code})")
                print(f"  {'â”€'*40}")
            
            # æ„å»ºæ•°æ®æ‘˜è¦
            data_summary = self._build_data_summary(stock, data_bundle)
            
            # è¿è¡Œè¾©è®º
            debate_result = self.debate_engine.run_debate(
                stock_code=code,
                company_name=name,
                data_summary=data_summary,
                quant_summary=quant_summary
            )
            
            output.debate_results[code] = debate_result
        
        # ç”Ÿæˆæœ€ç»ˆæ¨è
        output.final_recommendations = self._generate_recommendations(
            ranked_stocks[:max_stocks], output.debate_results
        )
        
        # ç»Ÿè®¡
        output.stats = {
            "stocks_analyzed": len(output.debate_results),
            "llm_providers": self.llm.get_available_providers(),
            "debate_rounds": self.max_debate_rounds,
            "recommendations": len(output.final_recommendations),
        }
        
        if self.verbose:
            print(f"\n  âœ… å†³ç­–å±‚å¤„ç†å®Œæˆ")
            print(f"     åˆ†æè‚¡ç¥¨: {output.stats['stocks_analyzed']} åª")
            print(f"     ä½¿ç”¨LLM: {output.stats['llm_providers']}")
        
        return output
    
    def _build_data_summary(self, stock: Dict, data_bundle) -> str:
        """æ„å»ºè‚¡ç¥¨æ•°æ®æ‘˜è¦"""
        lines = [f"## {stock.get('name', '')} ({stock.get('code', '')})"]
        
        lines.append(f"\n### é‡åŒ–ä¿¡å·")
        lines.append(f"- MLé¢„æµ‹ä¿¡å·: {stock.get('ml_signal', 'N/A')}")
        lines.append(f"- å› å­ç»¼åˆå¾—åˆ†: {stock.get('factor_score', 'N/A')}")
        lines.append(f"- ç»¼åˆæ’åå¾—åˆ†: {stock.get('combined_score', 'N/A')}")
        lines.append(f"- è¡Œä¸š: {stock.get('industry', 'N/A')}")
        lines.append(f"- æœ€æ–°ä»·æ ¼: {stock.get('latest_price', 'N/A')}")
        
        # ä»data_bundleè·å–æ›´å¤šä¿¡æ¯
        if data_bundle and hasattr(data_bundle, 'stock_universe'):
            stock_record = data_bundle.stock_universe.get(stock.get('code', ''))
            if stock_record and stock_record.financial_data:
                fd = stock_record.financial_data
                lines.append(f"\n### è´¢åŠ¡æŒ‡æ ‡")
                lines.append(f"- å¹´åŒ–æ”¶ç›Šç‡: {fd.get('annual_return', 0):.2%}")
                lines.append(f"- å¹´åŒ–æ³¢åŠ¨ç‡: {fd.get('annual_volatility', 0):.2%}")
                lines.append(f"- å¤æ™®æ¯”ç‡: {fd.get('sharpe_ratio', 0):.2f}")
                lines.append(f"- æœ€å¤§å›æ’¤: {fd.get('max_drawdown', 0):.2%}")
                lines.append(f"- è¿‘1æœˆæ”¶ç›Š: {fd.get('return_1m', 0):.2%}")
                lines.append(f"- è¿‘3æœˆæ”¶ç›Š: {fd.get('return_3m', 0):.2%}")
                lines.append(f"- è¿‘1å¹´æ”¶ç›Š: {fd.get('return_1y', 0):.2%}")
                lines.append(f"- 52å‘¨æœ€é«˜: {fd.get('price_52w_high', 0):.2f}")
                lines.append(f"- 52å‘¨æœ€ä½: {fd.get('price_52w_low', 0):.2f}")
        
        return "\n".join(lines)
    
    def _generate_recommendations(self, ranked_stocks: List[Dict],
                                    debate_results: Dict[str, DebateResult]) -> List[Dict]:
        """ç»¼åˆé‡åŒ–ä¿¡å·å’Œå®šæ€§åˆ†æç”Ÿæˆæœ€ç»ˆæ¨è"""
        recommendations = []
        
        for stock in ranked_stocks:
            code = stock.get('code', '')
            debate = debate_results.get(code)
            
            rec = {
                'code': code,
                'name': stock.get('name', code),
                'quant_score': stock.get('combined_score', 0),
                'qualitative_score': debate.final_score if debate else 5.0,
                'investment_rating': debate.investment_rating if debate else "å¾…åˆ†æ",
                'confidence': debate.final_confidence if debate else 0.5,
                'consensus': debate.consensus if debate else "",
                'bull_case': debate.bull_case if debate else "",
                'bear_case': debate.bear_case if debate else "",
                'valuation': debate.valuation_summary if debate else "",
            }
            
            # ç»¼åˆå¾—åˆ† (é‡åŒ–60% + å®šæ€§40%)
            rec['final_score'] = rec['quant_score'] * 0.6 + (rec['qualitative_score'] / 10) * 0.4
            
            recommendations.append(rec)
        
        # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
        recommendations.sort(key=lambda x: x['final_score'], reverse=True)
        
        return recommendations


# ==================== ä¾¿æ·å‡½æ•° ====================

def run_decision_analysis(ranked_stocks: List[Dict], data_bundle=None,
                           quant_summary: str = "", verbose: bool = True,
                           max_stocks: int = 5) -> DecisionLayerOutput:
    """ä¾¿æ·å‡½æ•°ï¼šè¿è¡Œå†³ç­–åˆ†æ"""
    layer = UnifiedDecisionLayer(verbose=verbose)
    return layer.analyze(ranked_stocks, data_bundle, quant_summary, max_stocks)


if __name__ == "__main__":
    print("=" * 60)
    print("V6.0 ç»Ÿä¸€å†³ç­–å±‚æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•LLMé€‚é…å™¨
    llm = LLMAdapter(verbose=True)
    print(f"\nå¯ç”¨LLM: {llm.get_available_providers()}")
    
    if llm.is_available():
        response = llm.chat([
            {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±"}
        ])
        print(f"\nLLMå“åº”: {response.content[:200]}")
        print(f"æä¾›å•†: {response.provider}, æ¨¡å‹: {response.model}")
