#!/usr/bin/env python3
"""
Quant-Investor V7.1 - å…¨å±€æ•°æ®åˆ†æï¼ˆ1900åªè‚¡ç¥¨ï¼‰
ä¼˜åŒ–ç‰ˆæœ¬ï¼šåˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜é—®é¢˜
"""

import sys
import os
from datetime import datetime
import pandas as pd
import numpy as np

sys.path.insert(0, '/root/.openclaw/workspace/myQuant/scripts/unified')

from stock_database import StockDatabase
from factor_analyzer import FactorAnalyzer
from enhanced_model_layer import EnhancedModelLayer
from macro_terminal_tushare import create_terminal
from risk_management_layer import RiskManagementLayer
from decision_layer import DecisionLayer
import warnings
warnings.filterwarnings('ignore')

print('=' * 80)
print('Quant-Investor V7.1 - å…¨å±€æ•°æ®åˆ†æ')
print('=' * 80)
print(f'æ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
print()

# ä»æ•°æ®åº“è·å–å·²ä¸‹è½½çš„è‚¡ç¥¨
db = StockDatabase()
stats = db.get_statistics()

print('ğŸ“Š æ•°æ®æ¦‚å†µ:')
print(f'  è‚¡ç¥¨æ€»æ•°: {stats["total_stocks"]}')
print(f'  å·²ä¸‹è½½: {stats["stocks_with_data"]} åª')
print(f'  æ•°æ®è®°å½•: {stats["total_records"]:,} æ¡')
print(f'  æ—¥æœŸèŒƒå›´: {stats["date_range"]}')
print()

# è·å–æ‰€æœ‰æœ‰æ•°æ®çš„è‚¡ç¥¨
import sqlite3
conn = sqlite3.connect(db.db_path)
cursor = conn.cursor()
cursor.execute('SELECT DISTINCT ts_code FROM daily_data')
all_stocks = [row[0] for row in cursor.fetchall()]
conn.close()

print(f'æœ¬æ¬¡åˆ†æ: {len(all_stocks)} åªè‚¡ç¥¨ï¼ˆå…¨å±€æ•°æ®ï¼‰')
print('=' * 80)
print()

# ========== ç¬¬1å±‚ï¼šæ•°æ®å±‚ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰==========
print('[Layer 1] æ•°æ®å±‚ - åŠ è½½å…¨å±€æ•°æ®...')

# ä»æ•°æ®åº“ç›´æ¥è¯»å–æ‰€æœ‰æ•°æ®
print('  ä»æ•°æ®åº“è¯»å–...')
conn = sqlite3.connect(db.db_path)

# ä½¿ç”¨è¶…æ—¶è®¾ç½®ï¼Œé¿å…å¡ä½
import signal

class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException("æ•°æ®è¯»å–è¶…æ—¶")

# è®¾ç½®5åˆ†é’Ÿè¶…æ—¶
signal.signal(signal.SIGALRM, timeout_handler)
signal.alarm(300)

try:
    df = pd.read_sql_query(
        "SELECT * FROM daily_data WHERE trade_date >= '20200101' LIMIT 2500000", 
        conn
    )
    signal.alarm(0)  # å–æ¶ˆè¶…æ—¶
except TimeoutException:
    print('  è¯»å–è¶…æ—¶ï¼Œä½¿ç”¨å·²æœ‰æ•°æ®...')
    df = pd.DataFrame()

conn.close()

if len(df) == 0:
    print('  ä»æ•°æ®åº“ç›´æ¥è¯»å–å¤±è´¥ï¼Œå°è¯•åˆ†æ‰¹è¯»å–...')
    # åˆ†æ‰¹è¯»å–
    chunks = []
    conn = sqlite3.connect(db.db_path)
    for i, chunk in enumerate(pd.read_sql_query(
        "SELECT * FROM daily_data WHERE trade_date >= '20200101'", 
        conn, 
        chunksize=500000
    )):
        print(f'    è¯»å–æ‰¹æ¬¡ {i+1}: {len(chunk)} æ¡')
        chunks.append(chunk)
        if i >= 4:  # æœ€å¤šè¯»å–250ä¸‡æ¡
            break
    conn.close()
    df = pd.concat(chunks, ignore_index=True)

print(f'  åŠ è½½å®Œæˆ: {len(df):,} æ¡è®°å½•, {df["ts_code"].nunique()} åªè‚¡ç¥¨')

# é‡å‘½ååˆ—ä»¥å…¼å®¹ç°æœ‰ä»£ç 
df = df.rename(columns={
    'ts_code': 'symbol',
    'trade_date': 'date',
    'open': 'open',
    'high': 'high',
    'low': 'low',
    'close': 'close',
    'volume': 'volume',
    'amount': 'amount'
})

# æ·»åŠ ç‰¹å¾å·¥ç¨‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
print('  ç‰¹å¾å·¥ç¨‹...')

# åŠ¨é‡å› å­
for period in [5, 10, 20, 60, 120]:
    df[f'return_{period}d'] = df.groupby('symbol')['close'].pct_change(period)

# æ³¢åŠ¨ç‡å› å­
for period in [20, 60, 120]:
    df[f'volatility_{period}d'] = df.groupby('symbol')['close'].pct_change().rolling(period).std().values * np.sqrt(252)

# æŠ€æœ¯æŒ‡æ ‡
# RSI
delta = df.groupby('symbol')['close'].diff()
gain = delta.where(delta > 0, 0).groupby(df['symbol']).transform(lambda x: x.rolling(14).mean())
loss = (-delta.where(delta < 0, 0)).groupby(df['symbol']).transform(lambda x: x.rolling(14).mean())
rs = gain / loss
df['rsi_14'] = 100 - (100 / (1 + rs))

# å‡çº¿åç¦»
for period in [5, 10, 20, 60]:
    ma = df.groupby('symbol')['close'].transform(lambda x: x.rolling(period).mean())
    df[f'ma_bias_{period}'] = (df['close'] - ma) / ma

# MACD
exp1 = df.groupby('symbol')['close'].transform(lambda x: x.ewm(span=12, adjust=False).mean())
exp2 = df.groupby('symbol')['close'].transform(lambda x: x.ewm(span=26, adjust=False).mean())
df['macd'] = exp1 - exp2
df['macd_signal'] = df.groupby('symbol')['macd'].transform(lambda x: x.ewm(span=9, adjust=False).mean())

# ç›®æ ‡å˜é‡ï¼šæœªæ¥5å¤©æ”¶ç›Š
df['label_return'] = df.groupby('symbol')['close'].pct_change(5).shift(-5)

# åˆ é™¤ç¼ºå¤±å€¼
df_clean = df.dropna()
print(f'  æ¸…ç†å: {len(df_clean):,} æ¡è®°å½•')

# ========== ç¬¬2å±‚ï¼šå› å­å±‚ ==========
print('\n[Layer 2] å› å­å±‚ - å› å­åˆ†æ...')

factor_cols = [c for c in df_clean.columns if c.startswith(('return_', 'volatility_', 'rsi_', 'macd_', 'ma_bias_'))]
print(f'  å› å­æ•°é‡: {len(factor_cols)}')
print(f'  å› å­åˆ—è¡¨: {factor_cols[:5]}...')

# ç®€åŒ–å› å­é€‰æ‹©ï¼šä½¿ç”¨æ‰€æœ‰å› å­
selected_factors = factor_cols[:10]
print(f'  é€‰ä¸­å› å­: {len(selected_factors)} ä¸ª')

# ========== ç¬¬3å±‚ï¼šæ¨¡å‹å±‚ ==========
print('\n[Layer 3] æ¨¡å‹å±‚ - æ¨¡å‹è®­ç»ƒ...')

# å‡†å¤‡æ•°æ®
model_df = df_clean[selected_factors + ['label_return']].dropna()
print(f'  è®­ç»ƒæ ·æœ¬: {len(model_df):,}')

if len(model_df) > 10000:
    # å¦‚æœæ•°æ®å¤ªå¤šï¼ŒæŠ½æ ·è®­ç»ƒ
    model_df = model_df.sample(n=50000, random_state=42)
    print(f'  æŠ½æ ·å: {len(model_df):,}')

X = model_df[selected_factors]
y = model_df['label_return']

# è®­ç»ƒç®€å•æ¨¡å‹
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print('  è®­ç»ƒ Random Forest...')
rf = RandomForestRegressor(n_estimators=50, max_depth=10, n_jobs=-1, random_state=42)
rf.fit(X_train, y_train)

# é¢„æµ‹
train_score = rf.score(X_train, y_train)
test_score = rf.score(X_test, y_test)
print(f'  è®­ç»ƒé›† RÂ²: {train_score:.4f}')
print(f'  æµ‹è¯•é›† RÂ²: {test_score:.4f}')

# ç‰¹å¾é‡è¦æ€§
importance = pd.DataFrame({
    'feature': selected_factors,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)
print(f'\n  ç‰¹å¾é‡è¦æ€§:\n{importance.head()}')

# ========== ç¬¬4å±‚ï¼šå®è§‚å±‚ ==========
print('\n[Layer 4] å®è§‚å±‚ - å¸‚åœºè¶‹åŠ¿...')

try:
    terminal = create_terminal('CN')
    macro_report = terminal.generate_risk_report()
    macro_signal = macro_report.overall_signal
    macro_risk = macro_report.overall_risk_level
    print(f'  å®è§‚ä¿¡å·: {macro_signal} {macro_risk}')
except Exception as e:
    print(f'  å®è§‚åˆ†æå¤±è´¥: {e}')
    macro_signal = 'ğŸŸ¡'
    macro_risk = 'ä¸­é£é™©'

# ========== ç¬¬5å±‚ï¼šé£æ§å±‚ ==========
print('\n[Layer 5] é£æ§å±‚ - é£é™©è¯„ä¼°...')

# è®¡ç®—ç»„åˆé£é™©
portfolio_returns = df_clean.groupby('date')['label_return'].mean().dropna()

if len(portfolio_returns) > 0:
    volatility = portfolio_returns.std() * np.sqrt(252)
    sharpe = portfolio_returns.mean() / portfolio_returns.std() * np.sqrt(252) if portfolio_returns.std() > 0 else 0
    
    # æœ€å¤§å›æ’¤
    cum_returns = (1 + portfolio_returns).cumprod()
    rolling_max = cum_returns.expanding().max()
    drawdown = (cum_returns - rolling_max) / rolling_max
    max_drawdown = drawdown.min()
    
    print(f'  å¹´åŒ–æ³¢åŠ¨ç‡: {volatility:.2%}')
    print(f'  æœ€å¤§å›æ’¤: {max_drawdown:.2%}')
    print(f'  å¤æ™®æ¯”ç‡: {sharpe:.2f}')
    
    # ä»“ä½å»ºè®®
    if macro_signal == 'ğŸ”´':
        position = 0.3
    elif macro_signal == 'ğŸŸ¡':
        position = 0.5
    elif macro_signal == 'ğŸŸ¢':
        position = 0.8
    else:
        position = 0.5
    
    print(f'  å»ºè®®ä»“ä½: {position:.0%}')
else:
    print('  æ•°æ®ä¸è¶³ï¼Œè·³è¿‡é£æ§è®¡ç®—')

# ========== ç¬¬6å±‚ï¼šå†³ç­–å±‚ ==========
print('\n[Layer 6] å†³ç­–å±‚ - æŠ•èµ„å»ºè®®...')

print('\n' + '=' * 80)
print('ğŸ¯ æœ€ç»ˆæŠ•èµ„å»ºè®®ï¼ˆåŸºäºå…¨å±€æ•°æ®åˆ†æï¼‰')
print('=' * 80)
print(f'ğŸ“… åˆ†ææ—¥æœŸ: {datetime.now().strftime("%Y-%m-%d")}')
print(f'ğŸ“Š æ•°æ®è§„æ¨¡: {len(all_stocks)} åªè‚¡ç¥¨, {len(df_clean):,} æ¡è®°å½•')
print(f'ğŸŒ å®è§‚ç¯å¢ƒ: {macro_signal} {macro_risk}')
print(f'ğŸ“ˆ æ¨¡å‹è¡¨ç°: æµ‹è¯•é›† RÂ² = {test_score:.4f}')
print(f'ğŸ’¼ ä»“ä½å»ºè®®: {position:.0%}è‚¡ç¥¨ + {1-position:.0%}ç°é‡‘')
print(f'ğŸ“Š å¤æ™®æ¯”ç‡: {sharpe:.2f}')
print()
print('ğŸ† é‡è¦å› å­æ’å:')
for i, row in importance.head(5).iterrows():
    print(f'  {i+1}. {row["feature"]}: {row["importance"]:.3f}')
print()
print('âš ï¸  é£é™©æç¤º:')
print('  - å¸‚åœºå¤„äºä¸­é£é™©åŒºé—´ï¼Œæ³¨æ„æ³¢åŠ¨')
print('  - å»ºè®®åˆ†æ•£æŠ•èµ„ï¼Œå•ç¥¨ä»“ä½ä¸è¶…è¿‡20%')
print('  - è®¾ç½®æ­¢æŸä¿æŠ¤ï¼Œæ§åˆ¶å›æ’¤')
print('=' * 80)

print('\nâœ… å…¨å±€æ•°æ®åˆ†æå®Œæˆï¼')
